## 训练你自己的LoRA

WebUI旨在使训练你自己的LoRA变得尽可能简单。它可以归结为几个简单的步骤：

### **步骤1**：制定计划。
- 你想使用哪个基座模型？你制作的LoRA必须与某个架构（如LLaMA-13B）匹配，不能转移到其他架构（如LLaMA-7B、StableLM等都是不同的）。同一模型的衍生版本（如LLaMA-13B的Alpaca微调版）可能可以转移，但即便如此，最好还是只在你计划使用的模型上训练。
- 你要把它训练成什么样子？你想让它学习真实信息，还是简单的格式，...？

### **步骤2**：收集数据集。
- 如果你使用类似于[Alpaca](https://github.com/gururise/AlpacaDataCleaned/blob/main/alpaca_data_cleaned.json)格式的数据集，WebUI的`格式化数据集`选项卡原生支持该格式，并有预制的格式化选项。
- 如果你使用的数据集与Alpaca格式不匹配，但使用相同的基本JSON结构，你可以通过复制`training/formats/alpaca-format.json`到新文件并[编辑其内容](#格式文件)来创建自己的格式文件。
- 如果你可以将数据集转换为简单的文本文件，那也可以！你可以使用`原始文本文件`选项卡进行训练。
    - 这意味着你可以例如直接复制/粘贴聊天记录/文档页面/任何你想要的内容，放入一个纯文本文件中，然后对其进行训练。
- 如果你使用不符合这种格式的结构化数据集，你可能需要找到外部方法来转换它 - 或者提出issue请求原生支持。

### **步骤3**：进行训练。
- **3.1**：加载WebUI和你的模型。
    - 确保你没有加载任何LoRA（除非你想为多LoRA使用场景进行训练）。
- **3.2**：点击顶部的`训练`标签页，然后点击`训练LoRA`子标签页。
- **3.3**：填写LoRA的名称，在数据集选项中选择你的数据集。
- **3.4**：根据你的偏好选择其他参数。参见[下面的参数](#参数)。
- **3.5**：点击`开始LoRA训练`，然后等待。
    - 对于大型数据集，可能需要几个小时，如果是小规模运行，可能只需几分钟。
    - 你可能想在过程中监控你的[损失值](#损失)。

### **步骤4**：评估你的结果。
- 在模型标签页下加载LoRA。
- 你可以在`聊天`标签页上试试刚训练好的模型，也可以使用`训练`标签页的`困惑度评估`子标签页。
- 如果你使用了`Save every n steps`选项，你可以从LoRA模型文件夹内的子文件夹中获取模型的先前副本并尝试它们。

### **步骤5**：如果不满意，重新训练。
- 在训练之前确保卸载LoRA。
- 你可以简单地从先前的训练继续 - 使用`从以下LoRA复制参数`选择你的LoRA，并编辑参数。注意，你不能更改已创建的LoRA的`秩`。
    - 如果你想从途中保存的检查点恢复，只需将检查点文件夹的内容复制到LoRA的文件夹中。
    - （注意：`adapter_model.bin`是保存实际LoRA内容的重要文件）。
    - 这将使学习率和步数重新开始。如果你想从中途恢复，你可以将学习率调整为日志中最后报告的学习率，并减少你的epochs。
- 或者，如果你愿意，你可以完全重新开始。
- 如果你的模型生成崩坏的输出，你可能需要重新开始并使用更低的学习率。
- 如果你的模型没有学习详细信息但你希望它学习，你可能需要运行更多的周期，或者你可能需要更高的秩。
- 如果你的模型强制执行了你不想要的格式，你可能需要调整你的数据集，或重新开始同时不要训练那么久。

## 格式文件

如果使用JSON格式的数据集，它们格式差不多应该像这样：

```json
[
    {
        "somekey": "somevalue",
        "key2": "value2"
    },
    {
        // 等等
    }
]
```

其中键（如上面的`somekey`、`key2`）是标准化的，并且在整个数据集中相对一致，而值（如`somevalue`、`value2`）包含实际你想要训练的内容。

对于Alpaca，键是`instruction`、`input`和`output`，其中`input`有时是空白的。

一个简单的Alpaca格式文件，用于作为聊天机器人使用：

```json
{
    "instruction,output": "User: %instruction%\nAssistant: %output%",
    "instruction,input,output": "User: %instruction%: %input%\nAssistant: %output%"
}
```

注意，键（如`instruction,output`）是数据集键的逗号分隔列表，而值是使用这些键的简单字符串，用`%%`包围。

例如，如果一个数据集有`"instruction": "answer my question"`，那么格式文件中的`User: %instruction%\n`将自动填充为`User: answer my question\n`。

如果你的输入有不同的键集，你可以制作自己的格式文件来匹配它。这种格式文件设计得尽可能简单，以便于根据你的需求进行编辑。

## 原始文本文件设置

当使用原始文本文件作为你的数据集时，文本会根据你的`截断长度`自动分割成块，你可以获得一些基本选项来配置它们。
- `重叠长度`是块之间重叠的长度。重叠块有助于防止模型学习奇怪的中句切割，而是学习从早期文本流动的连续句子。
- `优先换行剪切长度`表示将剪切下来的文本放到新行的最大字符距离。这样做有助于防止在句子中间起一新行或结束一行，防止模型学习随机切断句子。
- `硬剪切字符串`表示必须进行无重叠的硬剪切的字符串。默认为`\n\n\n`，意味着3个换行符。没有训练过的文本块会包含这个字符串。这可以让你在同一文本文件中插入不相关的文本部分，但仍然确保模型不会学到随意改变主题。

## 参数

每个参数的基本用途和功能在WebUI页面上都有文档说明，所以请在UI中阅读它们以了解你的选项。

也就是说，以下是你应该考虑的最重要的参数选择指南：

### VRAM

- 首先，你必须考虑你的VRAM够不够。
    - 通常，在默认设置下，使用默认参数进行训练的VRAM使用量与生成文本时（上下文为1000+个token）非常接近（即，如果你可以生成文本，你就可以训练LoRA）。
        - 注意：目前在4位猴子补丁中默认情况下更糟。将`微批量大小`减少到`1`以恢复到预期表现。
    - 如果你有多余的VRAM，设置更高的批量大小将使用更多VRAM，并换取更好的训练质量。
    - 如果你有大量数据，设置更高的截断长度可能会有益，但会消耗大量VRAM。如果你可以空出一些显存，可以试试将批量大小设置为`1`，看看你能把截断长度推高到多少。
    - 如果你的VRAM不足，降低批量大小或截断长度。
    - 不要害怕尝试看看会发生什么。反正调高了的话只会报错而已，你可以调低点再试一次。

### 秩

- 其次，你要考虑你想要的学习量。
    - 例如，你可能只想学习对话格式（就像Alpaca的情况），在这种情况下，设置低`秩`值（32或更低）效果很好。
    - 或者，你可能在训练项目文档，你希望机器人能理解并能回答相关问题，在这种情况下，秩越高越好。
    - 通常，更高的秩 = 更精确的学习 = 学习的总内容更多 = 训练时使用更多VRAM。

### 学习率和周期

- 第三，你希望它学习得有多仔细。
    - 换句话说，你对模型失去不相关的理解的程度是否可以接受。
    - 你可以通过3个关键设置来控制这一点：学习率、它的调度器和你的总周期数。
    - 学习率控制模型看到的每个词符对模型进行的改变量。
        - 它通常以科学记数法表示，例如`3e-4`意味着`3 * 10^-4`，即`0.0003`。`e-`后面的数字控制数字中有多少个`0`。
        - 更高的值让训练运行得更快，但也更有可能破坏模型中的先前数据。
    - 你基本上就两个变量需要平衡：学习率和周期。
        - 如果你提高学习率，你可以相应地降低周期来匹配。高学习率 + 低周期 = 非常快，低质量的训练。
        - 如果你降低学习率，设置高周期。低学习率 + 高周期 = 慢但高质量的训练。
    - 调度器控制训练过程中的变化程度————它开始比较高，然后变低。这有助于平衡获取数据和保持良好质量。
        - 你可以在这边的[HuggingFace文档](https://moon-ci-docs.huggingface.co/docs/transformers/pr_1/en/main_classes/optimizer_schedules#transformers.SchedulerType)看到不同调度器选项的图表

## 损失

当你运行训练时，WebUI的控制台窗口会记录报告，其中包括一个名为`损失`的数值。它会从一个高数字开始，然后随着时间的推移逐渐降低。

在AI训练领域，"损失"理论上意味着"模型离完美有多近"，`0`意味着"绝对完美"。这是通过测量模型输出你训练它输出的确切文本和它实际输出的文本之间的差异来计算的。

实际上，一个好的LLM应该在其人工大脑中有着天马行空的点子，所以损失为`0`表明模型已经崩溃，忘记了如何思考你训练它的内容以外的任何事情。

因此，实际上，损失是一个平衡游戏：你希望它足够低以理解你的数据，但又足够高以不忘记其他一切。通常，如果它低于`1.0`，它就会开始忘记它之前的记忆，你应该停止训练。在某些情况下，你可能更喜欢将它降低到`0.5`（如果你希望它的输出非常非常可被预测）。不同的目标有不同的需求，所以不要害怕，不断试验，看看什么最适合你。

注意：如果你看到损失从一开始就是或突然跳到精确的`0`，很可能你的训练过程出了问题（例如模型损坏）。

## 注意：4位猴子补丁

[4位LoRA猴子补丁](GPTQ-models-(4-bit-mode).md#using-loras-in-4-bit-mode)可以用于训练，但有副作用：
- VRAM使用量目前更高。你可以将`微批量大小`减少到`1`来补偿。
- 模型会做一些奇怪的事情。LoRA会自己应用，或拒绝应用，或自发地出错等。在训练/使用之间重新加载基座模型或重新启动WebUI可能有助于最小化任何出错的机会。
- 目前不支持同时加载或使用多个LoRA。
- 总的来说，要认识到并将这个猴子补丁当作它本来的样子 - 一个临时的脏脏黑客 - 它有用，但不是很稳定。当一切都合并到上游以获得完全官方支持时，它会随着时间的推移变得更好。
