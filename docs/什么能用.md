## 什么能用

| 加载器         | 加载1个LoRA | 加载2个或更多LoRAs | 训练LoRAs | 多模态模型扩展 | 困惑度评估 |
|----------------|----------------|-------------------------|----------------|----------------------|-----------------------|
| Transformers   |       ✅       |           ✅\*\*        |       ✅\*     |          ✅          |           ✅          |
| llama.cpp      |       ❌       |           ❌            |       ❌       |          ❌          |    请用 llamacpp_HF    |
| llamacpp_HF    |       ❌       |           ❌            |       ❌       |          ❌          |           ✅          |
| ExLlamav2_HF   |       ✅       |           ✅            |       ❌       |          ❌          |           ✅          |
| ExLlamav2      |       ✅       |           ✅            |       ❌       |          ❌          |   请用 ExLlamav2_HF    |
| AutoGPTQ       |       ✅       |           ❌            |       ❌       |          ✅          |           ✅          |
| AutoAWQ        |       ?        |           ❌            |       ?        |          ?           |           ✅          |
| HQQ            |       ?        |           ?             |       ?        |          ?           |           ✅          |

❌ = 还没实现

✅ = 已经实现

\* 用Transformers加载器也可以训练GPTQ模型的LoRA模型。不过在加载模型之前记得勾选“auto-devices”和“disable_exllama”。

\*\* PEFT中的Multi-LoRA非常棘手，并且目前的实现也并非在所有情况下可靠。
