## 生成标签页

包含控制文本生成的参数。

### 简要概述

大型语言模型（LLM）的工作原理是逐个生成词符。给定您的提示词，模型会计算每个可能的下一个词符的概率，然后才进行实际的词符生成。 

* 在 *贪婪解码* 中，总是选择最有可能的词符。
* 通常，*采样* 技术用于以更复杂的方式从下一个词符分布中选择词符，目的是提高生成文本的质量。

### 预设菜单

可以用来保存和加载参数组合以供重复使用。

* **🎲 按钮**：创建一个随机但合理的预设。对于这些类别：移除尾部词符、避免重复、以及整平分布来说，每个类别都只包含一个参数。也就是说，随机的结果中不会混合使用Top P和Top K，重复度惩罚因子和按出现频率的重复度惩罚因子也是一样的道理。您可以使用这个按钮来在多次尝试 “重新生成” 之后跳出糟糕的生成循环。

#### 内置预设

这些预设是通过一个名为 “Preset Arena” 的盲选比赛获得的，数百人参与了投票。完整的结果可以在[这里](https://github.com/oobabooga/oobabooga.github.io/blob/main/arena/results.md)找到。

一个关键的结论是，最好的预设是：

* **用于Instruct模式**：Divine Intellect、Big O、simple-1。
* **用于Chat模式**：Midnight Enigma、Yara、Shortwave。

其他的预设有：

* Mirostat：一个特殊的解码技术，首先在llama.cpp中实现，然后在这个存储库中为所有加载器进行了适配。许多人在聊天中使用它取得了积极的结果。
* LLaMA-Precise：一个在 Preset Arena 之前是 web UI 默认预设的旧版预设。
* Debug-deterministic：关闭采样。这对调试很有用，或者如果您故意想使用贪婪解码。

### 参数描述

如果想要了解更多关于参数的信息，可以查看[transformers文档](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig)。

* **最大新词符数（max_new_tokens）**：生成的最大词符数。不要设置得太高：它在公式 `(prompt_length) = min(truncation_length - max_new_tokens, prompt_length)` 中用于截断计算，所以如果设置得太高，您的提示词将被截断。
* **采样温度（temperature）**：控制输出的随机性的主要因子。值为 0 = 一成不变（只使用最可能的词符）。值越高随机性越大。
* **Top P（top_p）**：如果未设为1，概率加起来小于这个值的词符会被选择。值越高，可能的随机结果的范围越大。
* **Min P（min_p）**：概率小于 `(min_p) * (概率最高的词符的概率)` 的词符会被丢弃。这与 Top A 相同，但没有对概率进行平方。
* **Top K（top_k）**：类似于Top P，但是只选择最可能的Top K个词符。值越高，可能的随机结果的范围越大。
* **重复度惩罚因子（repetition_penalty）**：重复惩罚因子，用于惩罚先前出现过的词符。1表示没有惩罚，值越高，重复的可能性越小，值越低，重复的可能性越大。
* **按是否存在的重复度惩罚加数（presence_penalty）**：类似于重复度惩罚因子，但是是在原始词符分数上增加一个偏移量，而不是一个乘法因子。它可能会产生更好的结果。0表示没有惩罚，值越高，重复的可能性越小，值越低，重复的可能性越大。
* **按出现频率的重复度惩罚因子（frequency_penalty）**：基于词符在上下文中出现的次数进行缩放的重复度惩罚。使用这个时要小心；词符被惩罚的次数没有限制。
* **用于重复度惩罚计算的词符范围（repetition_penalty_range）**：用于重复度惩罚计算的词符范围。考虑最近的词符数量。0表示所有词符都会被考虑。
* **Typical P（typical_p）**：如果未设为1，在给定前文的情况下，只选择比随机词符有如此多可能出现的词符。
* **无尾采样超参数（tfs）**：尝试在分布中检测并移除尾部的低概率词符。详情请参见[这篇博客文章](https://www.trentonbricken.com/Tail-Free-Sampling/)。越接近0，丢弃的词符越多。
* **Top A（top_a）**：概率小于 `(top_a) * (概率最高的词符的概率)^2` 的词符会被丢弃。
* **ε截断（epsilon_cutoff）**：单位为1e-4；一个合理的值是3。这个值设置了一个概率下限，低于这个概率的词符不会被采样。
* **η截断（eta_cutoff）**：单位为1e-4；一个合理的值是3。这是特殊的η采样技术的主要参数。详情请参见[这篇论文](https://arxiv.org/pdf/2210.15191.pdf)。
* **指导比例（guidance_scale）**：无分类器指导（CFG）的主要参数。[这篇论文](https://arxiv.org/pdf/2306.17806.pdf)建议1.5是一个不错的值。它可以与负面提示词一起使用，也可以单独使用。
* **负面提示词（Negative prompt）**：只在指导比例不等于1时使用。它对于指导模型和自定义系统消息非常有用。您可以在这个字段中放置完整的提示词，将系统消息替换为模型的默认消息（如 “你是Llama，一个乐于助人的助手...”），以便模型更多地关注您的自定义系统消息。
* **惩罚系数α（penalty_alpha）**：将此参数设置为大于零并取消勾选 “使用采样算法” 即可启用对比搜索。此参数应和较低的 Top K 值一起使用，例如4。
* **mirostat模式（mirostat_mode）**：启用Mirostat采样技术。它旨在在采样过程中控制困惑度。详情请参见[这篇论文](https://arxiv.org/abs/2007.14966)。
* **mirostat参数τ（mirostat_tau）**: 不清楚，详见论文。根据 Preset Arena 的说法，8 是一个不错的值。
* **mirostat参数η（mirostat_eta）**: 不清楚，详见论文。根据 Preset Arena 的说法，0.1 是一个不错的值。
* **动态温度（dynamic_temperature）**: 启用动态温度。这会修改温度，使其范围在 "dynatemp_low"（最低值）和 "dynatemp_high"（最高值）之间，通过基于熵的缩放进行调整。曲线的陡度由 "dynatemp_exponent" 控制。
* **平滑因子（smoothing_factor）**: 启用二次采样。当 `0 < smoothing_factor < 1` 时，词符分布变得更加平坦。当 `smoothing_factor > 1` 时，分布变得更为尖锐。
* **温度采样放最后（temperature_last）**: 使温度成为最后一个采样器而不是第一个。这样您可以用一个采样器（如 min_p）去除低概率词符，然后使用高温度使模型保持创意性而不失连贯性。注意：此参数优先于“采样器优先级”。这意味着 `temperature` / `dynamic_temperature` / `quadratic_sampling` 会从它们所在的地方被移除并移动到堆栈的末尾。
* **使用采样算法（do_sample）**: 取消勾选时，完全禁用采样，改用贪婪解码（总是选择最可能的词符）。
* **种子（Seed）**: 将 Pytorch 的随机种子设置为该数值。注意，有些加载器不使用 Pytorch（特别是 llama.cpp），而且其他的加载器也不是确定性的（如 ExLlamaV2）。对于这些加载器，随机种子没有影响。
* **编码器重复惩罚（encoder_repetition_penalty）**: 也称为“幻觉过滤器”。用于惩罚未在先前文本中出现的词符。值越高，越有可能保持上下文，值越低，越有可能偏离上下文。
* **禁止重复的N元语法元数（no_repeat_ngram_size）**: 如果未设置为0，则指定完全禁止重复的词符集的长度。值越高=禁止更长的短语重复，值越低=禁止单词或字母的重复。在大多数情况下，只用0或高值是个好主意。

在右侧（如果您在使用移动设备，则在下方），存在以下参数：

* **将提示词截断至此长度**: 用于防止提示词长度超过模型的上下文长度。在transformers加载器的情况下，它可以用于动态分配内存，还可以用于设置VRAM上限，防止内存溢出。加载模型时，这个参数会自动更新为模型的上下文长度（对于使用这些参数的加载器，从 "n_ctx" 或 "max_seq_len" 中获取，对于不使用这些参数的加载器，从模型元数据中直接获取）。
* **每秒最多词符数（tokens/second）**: 使生成的文本可以实时可读，以防模型生成速度过快。如果您想展示您的GPU性能有多好，这是一个不错的选择。
* **自定义停止字符串**: 当生成到任何在此字段中设置的字符串时，模型将停止生成。注意，在聊天标签页生成文本时，默认情况下设置了一些停止字符串，如 "\nYour Name:" 和 "\nBot name:" 以用于聊天模式。这就是为什么该参数名称中带有“自定义”的原因。
* **禁用词符**: 允许您完全禁止模型生成某些词符。您需要在 "默认" > "词符" 或 "笔记本" > "词符" 下查找词符ID，或者直接查看模型的 `tokenizer.json` 文件。
* **自动确定最大新词符数（auto_max_new_tokens）**: 勾选后，最大新词符数 参数在后台扩展到可用的上下文长度。最大长度由 "将提示词截断至此长度" 参数决定。这对于在聊天标签页中获取长回复而无需多次点击“继续”非常有用。
* **禁用序列终止符**: 模型可以生成EOS（序列终止）词符。生成此词符后，生成过程会提前停止。勾选该参数时，该词符将被禁止生成，一次生成将始终生成“最大新词符数”个词符。
* **在提示词开头添加序列起始符**: 默认情况下，词符化器会在您的提示词中添加一个BOS（序列起始）词符。在训练期间，BOS词符用于区分不同的文档。如果取消勾选，则不会添加BOS词符，模型将把您的提示词理解为位于文档的中间而不是开始。这会显著改变输出，并使其更加有创意。
* **跳过特殊词符**: 在解码生成的词符时，跳过特殊词符的文本表示。否则，BOS会显示为`<s>`，EOS会显示为`</s>`，等等。
* **激活文本流式输出**: 取消勾选时，完整地响应一次输出，而不是逐个字地流式输出。如果您在高延迟网络上运行webui或使用`--share`，建议取消勾选此参数。
* **采样器优先级**: 允许您自定义不同采样器的应用顺序。列表中的第一个采样器首先被应用。通过这个，您可以定义像 `top_p -> temperature -> top_k` 这样的自定义顺序。
* **从文件加载语法**: 从 `text-generation-webui/grammars` 文件夹下的文件加载一个GBNF语法。输出写入到下面的“语法”框。您也可以使用这个菜单保存和删除自定义语法。
* **语法**: 允许您将模型输出限制为特定格式。例如，您可以使模型生成列表、JSON、特定单词等。语法非常强大，我强烈推荐使用它。语法最初看起来有点吓人，但一旦您理解它，它就变得非常容易。详情请参见[GBNF指南](https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md)。

## 角色

定义在“模式”下选择“chat”或“chat-instruct”时在聊天标签页中使用的角色的参数。

* **角色**: 一个下拉菜单，您可以从中选择保存的角色，保存一个新角色（💾按钮），以及删除所选角色（🗑️）。
* **用户名字**: 您的名字在提示词中显示的样子。
* **角色的名字**: 角色在提示词中显示的名字。
* **背景**: 一个始终位于提示词顶部的字符串。它永远不会被截断。它通常定义了角色的个性和对话的一些关键要素。
* **问候**: 角色的开场白。设置后，每当您开始新的聊天时它都会出现。
* **角色头像**: 角色的个人资料图片。为了使其生效，您需要点击💾保存角色。
* **您的头像**: 您的个人资料图片。它将用于所有对话中。

注意：在生成聊天提示词时，上下文和问候语字段中会进行以下替换：

* `{{char}}` 和 `<BOT>` 被替换为 “角色的名字”。
* `{{user}}` 和 `<USER>` 被替换为 “用户名字”。

因此您可以在角色定义中使用这些特殊的占位符。它们通常可以在TavernAI角色卡片中找到。

## 指令模板

定义在“模式”下选择“instruct”或“chat-instruct”时在聊天标签中使用的指令模板。

* **已保存的指令模板**: 一个下拉菜单，您可以从中加载已保存的模板，保存一个新模板（💾按钮），以及删除当前选择的模板（🗑️）。
* **自定义系统消息**: 一个定义角色个性的消息，替换其默认的“系统消息”字符串。示例：“你是一只鸭子。”
* **指令模板**: 一个定义用于指令对话格式的Jinja2模板。
* **发送至默认**: 将完整的指令模板以字符串格式发送到默认标签。
* **发送至笔记本**: 将完整的指令模板以字符串格式发送到笔记本标签。
* **发送至负面提示**: 将完整的指令模板以字符串格式发送到“参数”>“生成”下的“负面提示词”字段。
* **聊天模板**: 一个定义用于常规聊天对话格式的Jinja2模板。
* **chat-instruct模式下的指令**: 用于在聊天指令模式中要求模型扮演角色，以角色身份写一条回复的指令。可以创造性地用于生成特定类型的回复。

## 聊天记录

在此选项卡中，您可以下载当前的聊天历史记录（JSON格式），并上传之前保存的聊天历史记录。

上传历史记录后，会创建一个新聊天以容纳它。也就是说，您不会丢失聊天标签页中的当前聊天。

## 上传角色

### YAML 或 JSON

允许您上传使用web UI的YAML格式的角色，包括可选的角色头像。

### TavernAI PNG

允许您上传TavernAI角色卡。上传后会将其转换为web UI的内部YAML格式。
