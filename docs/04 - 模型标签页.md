这里是加载模型、将LoRA应用到已加载模型和下载新模型的地方。

## 模型加载器

### Transformers

可加载：全精度（16位或32位）模型。通常，其仓库名称中没有 GGUF、EXL2、GPTQ 或 AWQ，这类模型文件命名为 `pytorch_model.bin` 或 `model.safetensors`。

示例：[https://huggingface.co/lmsys/vicuna-7b-v1.5](https://huggingface.co/lmsys/vicuna-7b-v1.5)。

全精度模型使用大量的显存，因此通常需要选择“load_in_4bit”和“use_double_quant”选项，将模型加载为4位精度，使用bitsandbytes库。

该加载器还可以加载GPTQ模型并训练LoRA。在这种情况下，请在加载模型之前确保勾选“自动分配设备”和“禁用ExLlama”选项。

选项：

* **GPU内存（gpu-memory）**：设置为大于0时，使用accelerate库启用CPU卸载，一部分层会转移到CPU中。性能非常差。注意，accelerate对该参数的处理不是很精确，因此如果希望显存使用最多为10 GiB，可能需要将此参数设置为9 GiB或8 GiB。可以与“以8位量化加载”一起使用，但据我所知，不能与“以4位量化加载”一起使用。
* **CPU内存（cpu-memory）**：类似于上面的参数，您还可以设置CPU内存使用量的上限。不管是显存还是CPU内存不够用的部分将转移到磁盘缓存中，因此要使用此选项，您还需要勾选“磁盘”选项。
* **计算数据类型（compute_dtype）**：当勾选“以4位量化加载”时使用。我建议保持默认值。
* **量化类型（quant_type）**：当勾选“以4位量化加载”时使用。我建议保持默认值。
* **alpha值（alpha_value）**：用于在质量损失较小的情况下扩展模型的上下文长度。根据我的测量，1.75最适合1.5倍上下文，2.5最适合2倍上下文。即，alpha = 2.5 时，可以将4096上下文长度的模型扩展到8192上下文长度。
* **rope频率基数（rope_freq_base）**：最初是另一种写“alpha值”的方式，后来成为一些模型如CodeLlama的必需参数，这些模型在微调时设置为1000000，因此需要在加载时也设置为1000000。
* **压缩位置嵌入（compress_pos_emb）**：最早由[kaiokendev](https://kaiokendev.github.io/til)发现的上下文长度扩展方法。设置为2时，上下文长度加倍，为3时三倍，依此类推。应该只用于微调时将此参数设置为1以外值的模型。对于未调优以拥有更大上下文长度的模型，alpha值将导致较小的精度损失。
* **CPU**：使用Pytorch在CPU模式下加载模型。模型将以32位精度加载，因此需要大量内存。使用transformers的CPU推理比llama.cpp老，但有效，但速度很慢。注意：此参数在llama.cpp加载器中有不同的解释（见下文）。
* **以8位量化加载（load-in-8bit）**：使用bitsandbytes将模型加载为8位精度。该库中的8位内核已针对训练进行了优化，而不是推理，因此以8位量化加载比以4位量化加载慢（但更准确）。
* **bf16**：使用bfloat16精度而不是默认的float16。仅在不使用量化时适用。
* **自动分配设备（auto-devices）**：勾选后，后端会尝试猜测“gpu-memory”的合理值，以允许您使用CPU卸载加载模型。我建议手动设置“gpu-memory”。此参数在加载GPTQ模型时也需要勾选，此时需要在加载模型之前勾选。
* **磁盘（disk）**：为不适合GPU和CPU的层启用磁盘卸载。
* **以4位量化加载（load-in-4bit）**：使用bitsandbytes将模型加载为4位精度。
* **信任远程代码（trust-remote-code）**：一些模型使用自定义Python代码来加载模型或词符化器。对于此类模型，需要设置此选项。它不会下载任何远程内容：只会执行与模型一起下载的.py文件。这些文件可能包含恶意代码；我从未见过，但原则上有这种可能。
* **不使用快速模式（no_use_fast）**：不使用“fast”版本的词符化器。通常可以忽略；只有在无法加载模型的词符化器时才勾选此选项。
* **使用flash_attention 2（use_flash_attention_2）**：在加载模型时设置use_flash_attention_2=True。可能对训练有用。
* **禁用ExLlama（disable_exllama）**：仅当您通过transformers加载器加载GPTQ模型时适用。如果打算使用模型训练LoRA，则需要勾选此选项。

### ExLlamav2_HF

可加载：GPTQ和EXL2模型。EXL2模型名称中通常包含“EXL2”，而GPTQ模型名称中通常包含GPTQ，或者类似“-4bit-128g”的标识。

示例：

* https://huggingface.co/turboderp/Llama2-70B-exl2
* https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ

* **GPU分割（gpu-split）**：如果有多个GPU，应该在此字段中设置每个GPU要分配的内存量。确保为第一个GPU设置较低的值，因为缓存会在此分配。
* **最大序列长度（max_seq_len）**：模型的最大序列长度。在ExLlamaV2中，缓存是预分配的，因此此值越高，显存使用量越大。根据模型的元数据，它会自动设置为模型的最大序列长度，但您可能需要降低此值以使模型适应您的GPU。加载模型后，“提示词截断到此长度”参数会在“参数”>“生成”下自动设置为您选择的“最大序列长度”，因此不需要设置两次。
* **CFG缓存（cfg-cache）**：创建第二个缓存以保存CFG负面提示词。仅当且仅当您打算在“参数”>“生成”选项卡中使用CFG时需要设置此参数。勾选此参数会使缓存显存使用量翻倍。
* **不使用flash_attention（no_flash_attn）**：禁用flash attention。否则，只要库已安装，系统会自动使用。
* **8位缓存（cache_8bit）**：创建8位精度的缓存，而不是16位的。这可以节省显存，但会增加困惑度（我不确定具体增加多少）。
* **4位缓存（cache_4bit）**：使用分组量化创建Q4缓存。

### ExLlamav2

与ExLlamav2_HF相同，但使用ExLlamav2的内部采样器，而不是Transformers库中的采样器。

### AutoGPTQ

可加载：GPTQ模型。

* **权重位数（wbits）**：对于没有适当元数据的旧模型，手动设置模型精度。通常可以忽略。
* **组大小（groupsize）**：对于没有适当元数据的旧模型，手动设置模型组大小。通常可以忽略。
* **Triton**：仅在Linux上可用。需要同时使用act-order和组大小的模型。注意，ExLlamaV2可以在Windows上加载这些相同的模型，而无需triton。
* **不注入融合注意力（no_inject_fused_attention）**：提高性能，但增加显存使用量。
* **不注入融合MLP（no_inject_fused_mlp）**：类似于上一个参数，但仅适用于Triton。
* **不使用cuda_fp16（no_use_cuda_fp16）**：在某些系统上，不设置此选项可能会导致性能非常差。通常可以忽略。
* **按递减激活顺序量化（desc_act）**：对于没有适当元数据的旧模型，手动设置模型的“act-order”参数。通常可以忽略。

### llama.cpp

可加载：GGUF模型。注意：GGML模型已被废弃，不再工作。

示例：https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF

* **GPU层数（n-gpu-layers）**：分配到GPU的层数。设置为0时，只使用CPU。如果想要卸载所有层，只需将其设置为最大值。
* **n_ctx**：模型的上下文长度。在llama.cpp中，缓存是预分配的，因此此值越高，显存使用量越大。它会根据GGUF文件中的元数据自动设置为模型的最大序列长度，但您可能需要降低此值以使模型适应您的GPU。加载模型后，“提示词截断到此长度”参数会在“参数”>“生成”下自动设置为您选择的“n_ctx”，因此不需要设置两次。
* **张量分割（tensor_split）**：仅用于多GPU。以比例设置每个GPU的内存分配量。不要与其他加载器中的GB设置混淆；这里可以设置类似`30,70`的值，表示30%/70%的分配。
* **批处理大小（n_batch）**：提示词处理的批处理大小。更高的值应该使生成速度更快，但我从未从更改此值中获得任何收益。
* **线程数（threads）**：线程数。推荐值：物理核心数。
* **批处理线程数（threads_batch）**：批处理线程数。推荐值：总核心数（物理+虚拟）。
* **tensorcores**：使用带有“tensor cores”支持编译的llama.cpp，通常可以提高NVIDIA RTX显卡的性能。
* **streaming_llm**：实验性功能，用于避免在删除部分提示词时重新评估整个提示词，例如，当在聊天模式下达到模型的上下文长度并删除旧消息时。
* **CPU**：强制使用未启用GPU加速的llama.cpp版本。通常可以忽略。仅当想要使用CPU但llama.cpp无法正常工作时设置此选项。
* **禁用mul_mat_q（no_mul_mat_q）**：禁用mul_mat_q内核。此内核通常可以显著提高生成速度。包含此选项是为了在某些系统上不工作时可以禁用它。
* **不使用内存映射（no-mmap）**：一次性将模型加载到内存中，可能防止后续的I/O操作，但加载时间更长。
* **内存锁定**：强制系统将模型保留在内存中，而不是交换或压缩（不知道这是什么意思，从未使用过）。
* **NUMA**：可能会在某些多CPU系统上提高性能。

### llamacpp_HF

与llama.cpp相同，但使用transformers采样器，并使用transformers词符化器，而不是内部的llama.cpp词符化器。

要使用它，需要下载一个词符化器。有两种选择：

1) 在“下载模型或LoRA”下下载`oobabooga/llama-tokenizer`。这是默认的Llama词符化器。
2) 将您的.gguf文件与以下三个文件一起放在`models/`的子文件夹中：`tokenizer.model`、`tokenizer_config.json`和`special_tokens_map.json`。这将优先于选项1。

它有一个额外的参数：

* **全部应用Logit**：如果希望使用“训练”>“困惑度评估”选项卡评估llama.cpp模型的困惑度，需要勾选此选项。否则，保持未勾选状态，因为它会使提示词处理变慢。

### AutoAWQ

可加载：AWQ模型。

示例：https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v2-AWQ

参数整体上类似于AutoGPTQ。

## 模型下拉菜单

在这里可以选择要加载的模型、刷新可用模型列表(🔄)、加载/卸载/重新加载选定模型，并保存模型的设置。“设置”是下拉菜单下方输入字段（复选框、滑块、下拉菜单）中的值。

保存后，每当在下拉菜单中再次选择该模型时，这些设置会被恢复。

如果勾选了**自动加载模型**复选框，模型将在菜单中选中时立即加载。否则，需要点击“加载”按钮。

## LoRA下拉菜单

用于将LoRA应用到模型。请注意，并非所有加载器都实现了LoRA支持。详情请查看此[页面](https://github.com/Touch-Night/text-generation-webui/wiki)。

## 下载模型或LoRA

在这里可以直接通过镜像站[hf-mirror](https://hf-mirror.com/)从[https://huggingface.co/](https://huggingface.co/) 网站下载模型或LoRA。

* 模型将保存到`text-generation-webui/models`。
* LoRA将保存到`text-generation-webui/loras`。

在输入字段中，可以输入Hugging Face用户名/模型路径（如`facebook/galactica-125m`）或完整的模型URL（如`https://huggingface.co/facebook/galactica-125m`）。要指定分支，请在末尾添加一个“:”字符，如`facebook/galactica-125m:main`。

要下载单个文件（GGUF格式的模型需要这样），可以在输入字段中输入模型路径后点击“获取文件列表”，然后复制并粘贴所需文件名到“文件名”字段中，然后点击“下载”。
